{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65eed9d1",
   "metadata": {},
   "source": [
    "# NLP Project I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857bcb4",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743740b",
   "metadata": {},
   "source": [
    "Domain: Digital content management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06932633",
   "metadata": {},
   "source": [
    "Context: Text in the form of blogs, posts, articles, etc. are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0cfb7d",
   "metadata": {},
   "source": [
    "Data Description: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of 19,320  bloggers  gathered  from  blogger.com  in  August  2004.  The  corpus  incorporates  a  total  of  681,288  posts  and  over  140  million  words  -  or approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry, and astrological sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f71dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd2d69",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52816e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = pd.read_csv('blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c519d46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2c036",
   "metadata": {},
   "source": [
    "Data Analysis - Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9de5b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      681284 non-null  int64 \n",
      " 1   gender  681284 non-null  object\n",
      " 2   age     681284 non-null  int64 \n",
      " 3   topic   681284 non-null  object\n",
      " 4   sign    681284 non-null  object\n",
      " 5   date    681284 non-null  object\n",
      " 6   text    681284 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "blog.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3dccb7",
   "metadata": {},
   "source": [
    "Age and id have int datatype while the others - gender, topic, sign, date and text have object datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d90da43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7dbb3",
   "metadata": {},
   "source": [
    "The dataset is quite large with 680k+ records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac4b7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76553</th>\n",
       "      <td>1601316</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>12,April,2004</td>\n",
       "      <td>Sometimes I'm a cold hearted Bitch....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582219</th>\n",
       "      <td>4091880</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>12,August,2004</td>\n",
       "      <td>First posting ever to this blog.......We mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390312</th>\n",
       "      <td>3906531</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Education</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>06,July,2004</td>\n",
       "      <td>I finally joined in !!! !!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453725</th>\n",
       "      <td>1743209</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>08,August,2004</td>\n",
       "      <td>As you may have noticed - I have go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161967</th>\n",
       "      <td>2952149</td>\n",
       "      <td>male</td>\n",
       "      <td>13</td>\n",
       "      <td>Student</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>13,March,2004</td>\n",
       "      <td>Today was a pretty busy day. First my m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535620</th>\n",
       "      <td>3815511</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>03,July,2004</td>\n",
       "      <td>Today i cut myself on glass and bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160828</th>\n",
       "      <td>756402</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>19,August,2003</td>\n",
       "      <td>Well Neil left today. Everybody who's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416668</th>\n",
       "      <td>3644782</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Libra</td>\n",
       "      <td>08,April,2004</td>\n",
       "      <td>*Bleh* Things are ... slow at the momen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333672</th>\n",
       "      <td>3668625</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>13,July,2004</td>\n",
       "      <td>Jeepers! I think that time is playing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558938</th>\n",
       "      <td>2173787</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>09,November,2003</td>\n",
       "      <td>I spent my afternoon not doing the thin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age           topic         sign              date  \\\n",
       "76553   1601316  female   24          indUnk       Taurus     12,April,2004   \n",
       "582219  4091880    male   42  Transportation       Gemini    12,August,2004   \n",
       "390312  3906531    male   15       Education       Gemini      06,July,2004   \n",
       "453725  1743209    male   17         Student       Gemini    08,August,2004   \n",
       "161967  2952149    male   13         Student      Scorpio     13,March,2004   \n",
       "535620  3815511    male   14         Student  Sagittarius      03,July,2004   \n",
       "160828   756402  female   16          indUnk       Cancer    19,August,2003   \n",
       "416668  3644782  female   23     Engineering        Libra     08,April,2004   \n",
       "333672  3668625  female   25          indUnk  Sagittarius      13,July,2004   \n",
       "558938  2173787  female   17         Student    Capricorn  09,November,2003   \n",
       "\n",
       "                                                     text  \n",
       "76553              Sometimes I'm a cold hearted Bitch....  \n",
       "582219     First posting ever to this blog.......We mu...  \n",
       "390312              I finally joined in !!! !!!!           \n",
       "453725             As you may have noticed - I have go...  \n",
       "161967         Today was a pretty busy day. First my m...  \n",
       "535620              Today i cut myself on glass and bu...  \n",
       "160828         Well Neil left today. Everybody who's g...  \n",
       "416668         *Bleh* Things are ... slow at the momen...  \n",
       "333672         Jeepers! I think that time is playing t...  \n",
       "558938         I spent my afternoon not doing the thin...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78399a68",
   "metadata": {},
   "source": [
    "For analyzing the blogs, useful attributes are gender, age, topic, sign and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efbe415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indUnk                     251015\n",
       "Student                    153903\n",
       "Technology                  42055\n",
       "Arts                        32449\n",
       "Education                   29633\n",
       "Communications-Media        20140\n",
       "Internet                    16006\n",
       "Non-Profit                  14700\n",
       "Engineering                 11653\n",
       "Law                          9040\n",
       "Publishing                   7753\n",
       "Science                      7269\n",
       "Government                   6907\n",
       "Consulting                   5862\n",
       "Religion                     5235\n",
       "Fashion                      4851\n",
       "Marketing                    4769\n",
       "Advertising                  4676\n",
       "BusinessServices             4500\n",
       "Banking                      4049\n",
       "Chemicals                    3928\n",
       "Telecommunications           3891\n",
       "Accounting                   3832\n",
       "Military                     3128\n",
       "Museums-Libraries            3096\n",
       "Sports-Recreation            3038\n",
       "HumanResources               3010\n",
       "RealEstate                   2870\n",
       "Transportation               2326\n",
       "Manufacturing                2272\n",
       "Biotech                      2234\n",
       "Tourism                      1942\n",
       "LawEnforcement-Security      1878\n",
       "Architecture                 1638\n",
       "InvestmentBanking            1292\n",
       "Automotive                   1244\n",
       "Agriculture                  1235\n",
       "Construction                 1093\n",
       "Environment                   592\n",
       "Maritime                      280\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dae1d7",
   "metadata": {},
   "source": [
    "Topic being the Target attribute is skewed towards indUnk and student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e1202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      345193\n",
       "female    336091\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d09203",
   "metadata": {},
   "source": [
    "Gender attribute is quite balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e0800",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07003a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a1798",
   "metadata": {},
   "source": [
    "There seem to be no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8367dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\prash\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\prash\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce09aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dataset is large, use fewer records\n",
    "blog_df = blog.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f1ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e55a7",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c121c",
   "metadata": {},
   "source": [
    "Eliminate Non-English textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cbe7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ad5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df = blog_df[blog_df['text'].apply(detect_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5402a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e011aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595ae6e",
   "metadata": {},
   "source": [
    "Preprocess unstructured data to make it consumable for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c899e",
   "metadata": {},
   "source": [
    "Eliminate All special Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c45620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471789</th>\n",
       "      <td>3732266</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Government</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>YEhey.papalabas na ang long-awaited 'Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>3887270</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>25,July,2004</td>\n",
       "      <td>PHirST tyme for 'Cmoore' (haha), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90549</th>\n",
       "      <td>3113729</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>07,June,2004</td>\n",
       "      <td>[Saddle Creek] • May 25, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563566</th>\n",
       "      <td>3399714</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Military</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>06,July,2004</td>\n",
       "      <td>So, I really have nothing much to talk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603747</th>\n",
       "      <td>942828</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,January,2003</td>\n",
       "      <td>Two notes for those devoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age                 topic       sign  \\\n",
       "471789  3732266  female   17            Government     Cancer   \n",
       "3754    3887270  female   17               Student        Leo   \n",
       "90549   3113729    male   27  Communications-Media  Capricorn   \n",
       "563566  3399714    male   24              Military     Gemini   \n",
       "603747   942828  female   34                indUnk     Cancer   \n",
       "\n",
       "                   date                                               text  \n",
       "471789     24,June,2004         YEhey.papalabas na ang long-awaited 'Th...  \n",
       "3754       25,July,2004               PHirST tyme for 'Cmoore' (haha), ...  \n",
       "90549      07,June,2004                      [Saddle Creek] • May 25, 2...  \n",
       "563566     06,July,2004         So, I really have nothing much to talk ...  \n",
       "603747  24,January,2003                     Two notes for those devoted...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d310f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[^\\w ]\"\n",
    "blog_df.text = blog_df.text.apply(lambda s : re.sub(pattern,\"\",s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b11ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471789</th>\n",
       "      <td>3732266</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Government</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>YEheypapalabas na ang longawaited The n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>3887270</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>25,July,2004</td>\n",
       "      <td>PHirST tyme for Cmoore haha goin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90549</th>\n",
       "      <td>3113729</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>07,June,2004</td>\n",
       "      <td>Saddle Creek  May 25 2004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563566</th>\n",
       "      <td>3399714</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Military</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>06,July,2004</td>\n",
       "      <td>So I really have nothing much to talk a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603747</th>\n",
       "      <td>942828</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,January,2003</td>\n",
       "      <td>Two notes for those devoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age                 topic       sign  \\\n",
       "471789  3732266  female   17            Government     Cancer   \n",
       "3754    3887270  female   17               Student        Leo   \n",
       "90549   3113729    male   27  Communications-Media  Capricorn   \n",
       "563566  3399714    male   24              Military     Gemini   \n",
       "603747   942828  female   34                indUnk     Cancer   \n",
       "\n",
       "                   date                                               text  \n",
       "471789     24,June,2004         YEheypapalabas na ang longawaited The n...  \n",
       "3754       25,July,2004               PHirST tyme for Cmoore haha goin ...  \n",
       "90549      07,June,2004                      Saddle Creek  May 25 2004 ...  \n",
       "563566     06,July,2004         So I really have nothing much to talk a...  \n",
       "603747  24,January,2003                     Two notes for those devoted...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8575a",
   "metadata": {},
   "source": [
    "Lowercase all textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e686bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.text = blog_df.text.apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf36d7",
   "metadata": {},
   "source": [
    "Remove all Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f327cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwords in c:\\users\\prash\\anaconda3\\lib\\site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0261b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading all: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ceb359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords=set(stopwords.words('english'))\n",
    "blog_df.text = blog_df.text.apply(lambda s: ' '.join([words for words in s.split() if words not in stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f2275",
   "metadata": {},
   "source": [
    "Remove all extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da252fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.text = blog_df.text.apply(lambda s: s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51256239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471789</th>\n",
       "      <td>3732266</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Government</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>yeheypapalabas na ang longawaited notebook pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>3887270</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>25,July,2004</td>\n",
       "      <td>phirst tyme cmoore haha goin pacific mallasian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90549</th>\n",
       "      <td>3113729</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>07,June,2004</td>\n",
       "      <td>saddle creek may 25 2004 urllink good lifes of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563566</th>\n",
       "      <td>3399714</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Military</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>06,July,2004</td>\n",
       "      <td>really nothing much talk discuss really quiet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603747</th>\n",
       "      <td>942828</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>24,January,2003</td>\n",
       "      <td>two notes devoted venerable newman urllink mr ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age                 topic       sign  \\\n",
       "471789  3732266  female   17            Government     Cancer   \n",
       "3754    3887270  female   17               Student        Leo   \n",
       "90549   3113729    male   27  Communications-Media  Capricorn   \n",
       "563566  3399714    male   24              Military     Gemini   \n",
       "603747   942828  female   34                indUnk     Cancer   \n",
       "\n",
       "                   date                                               text  \n",
       "471789     24,June,2004  yeheypapalabas na ang longawaited notebook pri...  \n",
       "3754       25,July,2004  phirst tyme cmoore haha goin pacific mallasian...  \n",
       "90549      07,June,2004  saddle creek may 25 2004 urllink good lifes of...  \n",
       "563566     06,July,2004  really nothing much talk discuss really quiet ...  \n",
       "603747  24,January,2003  two notes devoted venerable newman urllink mr ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efb934",
   "metadata": {},
   "source": [
    "Build a base Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7449603",
   "metadata": {},
   "source": [
    "Create dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ea3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = blog_df['text']\n",
    "y = blog_df['topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17581cd1",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7458195",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28600b8c",
   "metadata": {},
   "source": [
    "Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c097983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83a2f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train)\n",
    "\n",
    "print('Vocabulary: ',cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "599e18b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 45',\n",
       " '00 bayern',\n",
       " '00 became',\n",
       " '00 chance',\n",
       " '00 couple',\n",
       " '00 denmark',\n",
       " '00 doesnt',\n",
       " '00 election',\n",
       " '00 extra',\n",
       " '00 horny',\n",
       " '00 innings',\n",
       " '00 less',\n",
       " '00 meters',\n",
       " '00 morning',\n",
       " '00 new',\n",
       " '00 oran',\n",
       " '00 pitch',\n",
       " '00 pretty',\n",
       " '00 really',\n",
       " '00 second',\n",
       " '00 seconds',\n",
       " '00 third',\n",
       " '00 well',\n",
       " '00 wouldnt',\n",
       " '000',\n",
       " '000 000',\n",
       " '000 023',\n",
       " '000 approximate',\n",
       " '000 attend',\n",
       " '000 bipolar',\n",
       " '000 cheers',\n",
       " '000 crore',\n",
       " '000 day',\n",
       " '000 duration',\n",
       " '000 era',\n",
       " '000 jobs',\n",
       " '000 lbs',\n",
       " '000 leaving',\n",
       " '000 lolol',\n",
       " '000 men',\n",
       " '000 mile',\n",
       " '000 oh',\n",
       " '000 peeps',\n",
       " '000 people',\n",
       " '000 placidus',\n",
       " '000 pounds',\n",
       " '000 someone',\n",
       " '000 songs',\n",
       " '000 square',\n",
       " '000 straight',\n",
       " '000 thats',\n",
       " '000 times',\n",
       " '0000',\n",
       " '0000 3847',\n",
       " '0000 mirage',\n",
       " '0000 new',\n",
       " '0000 wont',\n",
       " '00000',\n",
       " '00000 asked',\n",
       " '000000',\n",
       " '000000 1337',\n",
       " '000000 colour',\n",
       " '000000 even',\n",
       " '000000 fontsize',\n",
       " '000000 scrollbarhighlightcolor',\n",
       " '000000 urllink',\n",
       " '00000000',\n",
       " '00000000 eight',\n",
       " '00000000 kept',\n",
       " '000000000',\n",
       " '000000000 write',\n",
       " '0000000000',\n",
       " '0000000000 00000',\n",
       " '00000000000000000000000000000000000000000000000',\n",
       " '00000000000000000000000000000000000000000000000 bv',\n",
       " '0000000000000000001',\n",
       " '0000000000000000001 hahaha',\n",
       " '000000000000000001',\n",
       " '000000000000000001 eventually',\n",
       " '0000000000000000add',\n",
       " '0000000000000000add zeroes1',\n",
       " '0000000000000001000000001110000000000111000000000000011',\n",
       " '0000000000000001000000001110000000000111000000000000011 0101010000000000100000000001110000000000101100000000010',\n",
       " '0000000101010010111000000000000000000000110000000010000',\n",
       " '0000000101010010111000000000000000000000110000000010000 1100000000000101110000000010111100000000110111100000001',\n",
       " '0000001',\n",
       " '0000001 zbaras',\n",
       " '0000001000000001000001000000000100000100000000000001000',\n",
       " '0000001000000001000001000000000100000100000000000001000 0000001000011010010101010100101010100000001010100101000',\n",
       " '0000001000011010010101010100101010100000001010100101000',\n",
       " '0000001000011010010101010100101010100000001010100101000 1001010010010010000010100010100101010010100101010010101',\n",
       " '000000flickryourcomment',\n",
       " '000000flickryourcomment flickrframe',\n",
       " '000000this',\n",
       " '000000this certifies',\n",
       " '0000010000010000000000100000000000000000000100000000100',\n",
       " '0000010000010000000000100000000000000000000100000000100 0000001000000001000001000000000100000100000000000001000',\n",
       " '000001010100011000010110101101100101',\n",
       " '000001010100011000010110101101100101 01101110001000000111010001101000011001010010000001101',\n",
       " '00000188',\n",
       " '00000188 nbspnbspnbspnbspnbspnbspnbspnbspnbspnbsp',\n",
       " '000004',\n",
       " '000004 seconds',\n",
       " '00001',\n",
       " '00001 point',\n",
       " '0000100000101000101001000101001000000000010000001000010',\n",
       " '0000100000101000101001000101001000000000010000001000010 1010000100000000000000000000000000000100000000000000000',\n",
       " '000010111001001100101001000000',\n",
       " '000010111001001100101001000000 11110010110111101110101001000000111010001100001011011',\n",
       " '0000111000000000000000001010101010000000000000001010001',\n",
       " '0000111000000000000000001010101010000000000000001010001 0000100000101000101001000101001000000000010000001000010',\n",
       " '000032',\n",
       " '000032 afootertitlelink',\n",
       " '000032 fontfamily',\n",
       " '000032 textdecoration',\n",
       " '000050',\n",
       " '000050 sys5restart',\n",
       " '000066',\n",
       " '000066 one',\n",
       " '0001',\n",
       " '0001 model',\n",
       " '0001001010011011110110100001101110001000000100110101',\n",
       " '0001001010011011110110100001101110001000000100110101 100001011000110100100001100101011011100111001001111001',\n",
       " '0001101011011010010110111001100',\n",
       " '0001101011011010010110111001100 11100100000011000010110001001101111011101010111010000',\n",
       " '000124',\n",
       " '000124 lineproto5updown',\n",
       " '00015',\n",
       " '00015 better',\n",
       " '000154',\n",
       " '000154 sys5config_i',\n",
       " '000214',\n",
       " '000214 link3updown',\n",
       " '000215',\n",
       " '000215 lineproto5updown',\n",
       " '000320',\n",
       " '000320 sys5config_i',\n",
       " '0005',\n",
       " '0005 drastically',\n",
       " '0007154615',\n",
       " '0007154615 shields',\n",
       " '000911',\n",
       " '000911 120804',\n",
       " '001',\n",
       " '001 chance',\n",
       " '001 duped',\n",
       " '001 first',\n",
       " '001 instead',\n",
       " '001 name',\n",
       " '001 phone',\n",
       " '001 seconds',\n",
       " '001 transactions',\n",
       " '001 travel',\n",
       " '00100000',\n",
       " '00100000 00111010',\n",
       " '00100000 01000100',\n",
       " '00100000 01001001',\n",
       " '00100000 01100010',\n",
       " '00100000 01100011',\n",
       " '00100000 01100110',\n",
       " '00100000 01100111',\n",
       " '00100000 01101000',\n",
       " '00100000 01101001',\n",
       " '00100000 01101010',\n",
       " '00100000 01101011',\n",
       " '00100000 01101100',\n",
       " '00100000 01101101',\n",
       " '00100000 01110010',\n",
       " '00100000 01110011',\n",
       " '00100000 01110100',\n",
       " '00100000 01110111',\n",
       " '00100000 01111001',\n",
       " '0010000010001010000010000101010000010010010010000101001',\n",
       " '0010000010001010000010000101010000010010010010000101001 dont',\n",
       " '00101001',\n",
       " '00101100',\n",
       " '00101100 00100000',\n",
       " '00101100 01100101',\n",
       " '00101110',\n",
       " '00101110 00100000',\n",
       " '00110001',\n",
       " '00110001 much',\n",
       " '00110010',\n",
       " '00110010 00110011',\n",
       " '00110011',\n",
       " '00110011 00111000',\n",
       " '00110100101100001011101000110010101100100001111110010',\n",
       " '00110100101100001011101000110010101100100001111110010 000001010100011000010110101101100101',\n",
       " '00111000',\n",
       " '00111000 00110001',\n",
       " '0011100100011101010101010000010100011101000101001000010010000100100001',\n",
       " '0011100100011101010101010000010100011101000101001000010010000100100001 gordon',\n",
       " '00111010',\n",
       " '00111010 00101001',\n",
       " '00111111',\n",
       " '00111111 00100000',\n",
       " '00125',\n",
       " '00125 sec',\n",
       " '0015',\n",
       " '0015 got',\n",
       " '002',\n",
       " '002 currently',\n",
       " '002 middle',\n",
       " '002 spell',\n",
       " '002 worth',\n",
       " '00204',\n",
       " '00204 pages',\n",
       " '003',\n",
       " '003 date',\n",
       " '003 im',\n",
       " '003 last',\n",
       " '0030',\n",
       " '0030 thats',\n",
       " '0030hrs',\n",
       " '0030hrs accident',\n",
       " '0033',\n",
       " '0033 please',\n",
       " '003366',\n",
       " '003366 000066',\n",
       " '0034',\n",
       " '0034 659',\n",
       " '004',\n",
       " '004 interest',\n",
       " '004 male',\n",
       " '004 nicknames',\n",
       " '004 nicknamesandi',\n",
       " '004 went',\n",
       " '00404',\n",
       " '00404 pages',\n",
       " '005',\n",
       " '005 astrological',\n",
       " '005 gender',\n",
       " '005 million',\n",
       " '005 percent',\n",
       " '006',\n",
       " '006 age',\n",
       " '006 birthday',\n",
       " '006 nicknames',\n",
       " '0060504080',\n",
       " '0060504080 subjects',\n",
       " '0060509406',\n",
       " '0060509406 love',\n",
       " '0060521996',\n",
       " '0060529709',\n",
       " '0060529709 everything',\n",
       " '0060930535',\n",
       " '0060930535 fascinating',\n",
       " '0060934417',\n",
       " '0060934417 bel',\n",
       " '0060935332',\n",
       " '0060935332 emotionally',\n",
       " '0060935464',\n",
       " '0060935464 one',\n",
       " '0060958073',\n",
       " '0060958073 population',\n",
       " '0060959037',\n",
       " '0060959037 easy',\n",
       " '0060987103',\n",
       " '0060987103 maguire',\n",
       " '0064',\n",
       " '0064 identified',\n",
       " '0064 shall',\n",
       " '007',\n",
       " '007 also',\n",
       " '007 birthday',\n",
       " '007 bit',\n",
       " '007 blogging',\n",
       " '007 die',\n",
       " '007 gets',\n",
       " '007 go',\n",
       " '007 height',\n",
       " '007 ive',\n",
       " '007 jersey',\n",
       " '007 listed',\n",
       " '007 mmmm',\n",
       " '007 movie',\n",
       " '007 occupation',\n",
       " '007 octal',\n",
       " '007 see',\n",
       " '007 sharing',\n",
       " '007 terrorist',\n",
       " '007 theme',\n",
       " '007 turok',\n",
       " '007 twangguitar',\n",
       " '007 would',\n",
       " '007111',\n",
       " '00734',\n",
       " '00734 wait',\n",
       " '007s',\n",
       " '007s dentist',\n",
       " '007well',\n",
       " '007well really',\n",
       " '008',\n",
       " '008 bought',\n",
       " '008 hair',\n",
       " '008 height',\n",
       " '008 tried',\n",
       " '0082',\n",
       " '0082 great',\n",
       " '009',\n",
       " '009 android',\n",
       " '009 cn',\n",
       " '009 contacts',\n",
       " '009 eye',\n",
       " '009 hair',\n",
       " '009 weight',\n",
       " '009 zero',\n",
       " '00a53c',\n",
       " '00a53c colours',\n",
       " '00am',\n",
       " '00am nomination',\n",
       " '00but',\n",
       " '00but 2nd',\n",
       " '00ff00',\n",
       " '00ff00 00a53c',\n",
       " '00h00',\n",
       " '00h00 start',\n",
       " '00nbsp',\n",
       " '00nbsp gonna',\n",
       " '00nbsp nbsp',\n",
       " '00ryan',\n",
       " '00ryan id',\n",
       " '00s',\n",
       " '00s abc',\n",
       " '00well',\n",
       " '00well wanted',\n",
       " '00withoutactivedirectoryexe',\n",
       " '00withoutactivedirectoryexe hck',\n",
       " '01',\n",
       " '01 03',\n",
       " '01 05',\n",
       " '01 09',\n",
       " '01 105830',\n",
       " '01 115',\n",
       " '01 11th',\n",
       " '01 12',\n",
       " '01 120452',\n",
       " '01 120515',\n",
       " '01 120520',\n",
       " '01 120526',\n",
       " '01 120532',\n",
       " '01 120536',\n",
       " '01 120546',\n",
       " '01 13',\n",
       " '01 14',\n",
       " '01 15',\n",
       " '01 15pm',\n",
       " '01 16',\n",
       " '01 190226',\n",
       " '01 2001',\n",
       " '01 2002',\n",
       " '01 2003',\n",
       " '01 2004',\n",
       " '01 2004wimbledon',\n",
       " '01 203',\n",
       " '01 22',\n",
       " '01 2k3',\n",
       " '01 309',\n",
       " '01 311',\n",
       " '01 357',\n",
       " '01 44',\n",
       " '01 47',\n",
       " '01 81512',\n",
       " '01 81538',\n",
       " '01 81616',\n",
       " '01 81623',\n",
       " '01 81626',\n",
       " '01 81638',\n",
       " '01 81645',\n",
       " '01 81646',\n",
       " '01 81650',\n",
       " '01 ahahahahha',\n",
       " '01 airs',\n",
       " '01 also',\n",
       " '01 another',\n",
       " '01 answering',\n",
       " '01 appeal',\n",
       " '01 appears',\n",
       " '01 aquarius',\n",
       " '01 asian',\n",
       " '01 assuming',\n",
       " '01 bacteria',\n",
       " '01 beat',\n",
       " '01 best',\n",
       " '01 better',\n",
       " '01 big',\n",
       " '01 blardy',\n",
       " '01 blog',\n",
       " '01 blue',\n",
       " '01 bob',\n",
       " '01 bright',\n",
       " '01 bugger',\n",
       " '01 care',\n",
       " '01 certain',\n",
       " '01 chance',\n",
       " '01 charlize',\n",
       " '01 clearer',\n",
       " '01 color',\n",
       " '01 computer',\n",
       " '01 cranked',\n",
       " '01 cried',\n",
       " '01 cuz',\n",
       " '01 dad',\n",
       " '01 damn',\n",
       " '01 dashboard',\n",
       " '01 decided',\n",
       " '01 decline',\n",
       " '01 desk',\n",
       " '01 desperately',\n",
       " '01 didnt',\n",
       " '01 digital',\n",
       " '01 doesnt',\n",
       " '01 dont',\n",
       " '01 drink',\n",
       " '01 drugs',\n",
       " '01 dubya',\n",
       " '01 dude',\n",
       " '01 dudes',\n",
       " '01 eh',\n",
       " '01 eleventaking',\n",
       " '01 erotica_writings',\n",
       " '01 escapologyrobbie',\n",
       " '01 even',\n",
       " '01 every',\n",
       " '01 ew',\n",
       " '01 except',\n",
       " '01 fantastic',\n",
       " '01 felt',\n",
       " '01 fighting',\n",
       " '01 find',\n",
       " '01 first',\n",
       " '01 forgot',\n",
       " '01 forgotten',\n",
       " '01 friends',\n",
       " '01 fuck',\n",
       " '01 fucked',\n",
       " '01 fucking',\n",
       " '01 full',\n",
       " '01 gas',\n",
       " '01 get',\n",
       " '01 gets',\n",
       " '01 go',\n",
       " '01 god',\n",
       " '01 good',\n",
       " '01 goodnesszor',\n",
       " '01 goon',\n",
       " '01 greece',\n",
       " '01 hahaahahahah',\n",
       " '01 hahahahahahaha',\n",
       " '01 hahahahahhahah',\n",
       " '01 hahahahh',\n",
       " '01 hair',\n",
       " '01 havent',\n",
       " '01 head',\n",
       " '01 heh',\n",
       " '01 hehehe',\n",
       " '01 held',\n",
       " '01 hes',\n",
       " '01 hilarious',\n",
       " '01 hm',\n",
       " '01 holy',\n",
       " '01 human',\n",
       " '01 im',\n",
       " '01 images',\n",
       " '01 iowa',\n",
       " '01 jesse',\n",
       " '01 kiddie',\n",
       " '01 kirsten',\n",
       " '01 knew',\n",
       " '01 know',\n",
       " '01 known',\n",
       " '01 lame',\n",
       " '01 laughter',\n",
       " '01 law',\n",
       " '01 learned',\n",
       " '01 leg',\n",
       " '01 let',\n",
       " '01 like',\n",
       " '01 living',\n",
       " '01 lol',\n",
       " '01 lose',\n",
       " '01 lotr',\n",
       " '01 maledorito',\n",
       " '01 man',\n",
       " '01 med',\n",
       " '01 meh',\n",
       " '01 mel',\n",
       " '01 miles',\n",
       " '01 miss',\n",
       " '01 mralexpants',\n",
       " '01 national',\n",
       " '01 need',\n",
       " '01 new',\n",
       " '01 nick',\n",
       " '01 nope',\n",
       " '01 october',\n",
       " '01 officially',\n",
       " '01 oh',\n",
       " '01 ok',\n",
       " '01 omarosa',\n",
       " '01 one',\n",
       " '01 online',\n",
       " '01 opened',\n",
       " '01 owning',\n",
       " '01 people',\n",
       " '01 person',\n",
       " '01 phil',\n",
       " '01 pink',\n",
       " '01 pisses',\n",
       " '01 play',\n",
       " '01 played',\n",
       " '01 plus',\n",
       " '01 power',\n",
       " '01 priuses',\n",
       " '01 probably',\n",
       " '01 problems',\n",
       " '01 public',\n",
       " '01 quality',\n",
       " '01 razor',\n",
       " '01 read',\n",
       " '01 realized',\n",
       " '01 really',\n",
       " '01 reformat',\n",
       " '01 reintroduce',\n",
       " '01 role',\n",
       " '01 scary',\n",
       " '01 scrape',\n",
       " '01 see',\n",
       " '01 share',\n",
       " '01 shit',\n",
       " '01 silent',\n",
       " '01 sisters',\n",
       " '01 smart',\n",
       " '01 song',\n",
       " '01 special',\n",
       " '01 spidey',\n",
       " '01 spitfire',\n",
       " '01 sumner',\n",
       " '01 supposing',\n",
       " '01 sushi',\n",
       " '01 take',\n",
       " '01 televive',\n",
       " '01 thats',\n",
       " '01 think',\n",
       " '01 thunderstorms',\n",
       " '01 time',\n",
       " '01 told',\n",
       " '01 tooth',\n",
       " '01 totals',\n",
       " '01 tuition',\n",
       " '01 turned',\n",
       " '01 twosided',\n",
       " '01 underworld',\n",
       " '01 uni',\n",
       " '01 ur',\n",
       " '01 urllink',\n",
       " '01 way',\n",
       " '01 wedding',\n",
       " '01 well',\n",
       " '01 wheres',\n",
       " '01 whoa',\n",
       " '01 woman',\n",
       " '01 wont',\n",
       " '01 worry',\n",
       " '01 would',\n",
       " '01 wow',\n",
       " '01 wraths',\n",
       " '01 yeah',\n",
       " '01 yeeching',\n",
       " '01 yep',\n",
       " '01 yes',\n",
       " '01 yesssss',\n",
       " '01 younger',\n",
       " '01 youre',\n",
       " '01 zing',\n",
       " '010',\n",
       " '010 eye',\n",
       " '010 hair',\n",
       " '010 months',\n",
       " '010 told',\n",
       " '010 wear',\n",
       " '0100',\n",
       " '0100 0400',\n",
       " '0100 constructiveness',\n",
       " '0100 got',\n",
       " '0100 hrs',\n",
       " '0100 rationality',\n",
       " '0100 sedfsober',\n",
       " '0100 sleep',\n",
       " '0100 topic',\n",
       " '0100 wokeup',\n",
       " '01000001',\n",
       " '01000001 binary',\n",
       " '01000100',\n",
       " '01000100 01101111',\n",
       " '0100011001001111010101010100110000100000010011000100000101',\n",
       " '0100011001001111010101010100110000100000010011000100000101 0011100100011101010101010000010100011101000101001000010010000100100001',\n",
       " '01000111010001100101011100100010000001100001011000100',\n",
       " '01000111010001100101011100100010000001100001011000100 110111101110101011101000010000001110010011001010110001',\n",
       " '01001000000110110001100101011001110010',\n",
       " '01001000000110110001100101011001110010 11100010111000101110',\n",
       " '010010000110010101110010011001010010110000',\n",
       " '010010000110010101110010011001010010110000 1000000101100101101111011101010010000001100111011011',\n",
       " '01001001',\n",
       " '01001001 01100110',\n",
       " '01001111',\n",
       " '01001111 01101000',\n",
       " '0100111101101110011001010010000001110100',\n",
       " '0100111101101110011001010010000001110100 01101111001000000111010001101000011001',\n",
       " '010081407',\n",
       " '010081407 sure',\n",
       " '0100m',\n",
       " '0100m sprint',\n",
       " '0100pm',\n",
       " '0100pm back',\n",
       " '0101010000000000100000000001110000000000101100000000010',\n",
       " '0101010000000000100000000001110000000000101100000000010 0000000101010010111000000000000000000000110000000010000',\n",
       " '0101011101101000011000010111010000100000011',\n",
       " '0101011101101000011000010111010000100000011 000010111001001100101001000000',\n",
       " '010103',\n",
       " '010103 barcelona',\n",
       " '010111000100000010010000110000101110',\n",
       " '010111000100000010010000110000101110 01100100000011010000110010100100000011000100110010101',\n",
       " '0102',\n",
       " '0102 cireofnamellef',\n",
       " '0102 corntowm',\n",
       " '0102 right',\n",
       " '010204',\n",
       " '010204 amounted',\n",
       " '010204 release',\n",
       " '0103',\n",
       " '0103 cireofnamellef',\n",
       " '0103 corntowm',\n",
       " '0103 ltcherishthelovegt',\n",
       " '0104',\n",
       " '0104 cireofnamellef',\n",
       " '0104 corntowm',\n",
       " '0104 japan',\n",
       " '0104 ltsteelburngt',\n",
       " '0104 nickchannel',\n",
       " '010404always',\n",
       " '010404always loving',\n",
       " '0105',\n",
       " '0105 cireofnamellef',\n",
       " '0105 corntowm',\n",
       " '010504',\n",
       " '010504 may',\n",
       " '010504 storage',\n",
       " '0106',\n",
       " '0106 cireofnamellef',\n",
       " '0107',\n",
       " '0107 cireofnamellef',\n",
       " '010704',\n",
       " '010704 borders',\n",
       " '01072004',\n",
       " '01072004 sporttelegraphcouk',\n",
       " '0108',\n",
       " '0108 cireofnamellef',\n",
       " '010804',\n",
       " '010804 bit',\n",
       " '0109',\n",
       " '0109 urllink',\n",
       " '011',\n",
       " '011 braces',\n",
       " '011 ethnicity',\n",
       " '011 eye',\n",
       " '011 least',\n",
       " '011 race',\n",
       " '01100001',\n",
       " '01100001 01110100',\n",
       " '01100001 problem',\n",
       " '01100001011010010111010000100000011',\n",
       " '01100001011010010111010000100000011 10111011010000110100101101100011001010010000001110111',\n",
       " '01100010',\n",
       " '01100010 01110010',\n",
       " '01100011',\n",
       " '01100011 01101111',\n",
       " '01100100',\n",
       " '01100100 01100101',\n",
       " '01100100000011010000110010100100000011000100110010101',\n",
       " '01100100000011010000110010100100000011000100110010101 10010101101110001000000110100101101110011010010111010',\n",
       " '01100101',\n",
       " '01100101 00100000',\n",
       " '01100101 00101110',\n",
       " '01100101 01100101',\n",
       " '01100101 01101011',\n",
       " '01100101 01110011',\n",
       " '011001010010000001110000011011110111',\n",
       " '011001010010000001110000011011110111 01000111010001100101011100100010000001100001011000100',\n",
       " '01100110',\n",
       " '01100110 00100000',\n",
       " '01100110 01110101',\n",
       " '01100111',\n",
       " '01100111 01100101',\n",
       " '01101000',\n",
       " '01101000 00100000',\n",
       " '01101000 01100001',\n",
       " '01101000 01101000',\n",
       " '01101000 01101110',\n",
       " '01101000 01110010',\n",
       " '01101000 01110100',\n",
       " '01101000 01111001',\n",
       " '01101001',\n",
       " '01101001 00101100',\n",
       " '01101001 01110011',\n",
       " '01101001 01110100',\n",
       " '01101010',\n",
       " '01101010 01110101',\n",
       " '01101011',\n",
       " '01101011 00101100',\n",
       " '01101011 01101110',\n",
       " '01101100',\n",
       " '01101100 01101001',\n",
       " '01101101',\n",
       " '01101101 01100101',\n",
       " '01101110',\n",
       " '01101110 00100000',\n",
       " '01101110 01100101',\n",
       " '01101110 01101000',\n",
       " '01101110 01101111',\n",
       " '01101110 01111001',\n",
       " '01101110001000000111010001101000011001010010000001101',\n",
       " '01101110001000000111010001101000011001010010000001101 111011000010111010001101000001111110000110100001010',\n",
       " '01101111',\n",
       " '01101111 00100000',\n",
       " '01101111 00101100',\n",
       " '01101111 01100100',\n",
       " '01101111 01100101',\n",
       " '01101111 01101110',\n",
       " '01101111 01110101',\n",
       " '01101111 01110111',\n",
       " '01101111001000000111010001101000011001',\n",
       " '01101111001000000111010001101000011001 01001000000110110001100101011001110010',\n",
       " '01110010',\n",
       " '01110010 01100101',\n",
       " '01110010 01101001',\n",
       " '01110010 01101110',\n",
       " '01110010 01110011',\n",
       " '01110010 01111001',\n",
       " '01110011',\n",
       " '01110011 00100000',\n",
       " '01110011 00111111',\n",
       " '01110011 01101000',\n",
       " '01110011 01101111',\n",
       " '01110011 01110100',\n",
       " '01110100',\n",
       " '01110100 00100000',\n",
       " '01110100 01100101',\n",
       " '01110100 01101000',\n",
       " '01110100 01101111',\n",
       " '01110101',\n",
       " '01110101 00100000',\n",
       " '01110101 01101110',\n",
       " '01110101 01110011',\n",
       " '01110111',\n",
       " '01110111 00100000',\n",
       " '01110111 01101000',\n",
       " '01110111 01110010',\n",
       " '01111001',\n",
       " '01111001 00100000',\n",
       " '01111001 01101111',\n",
       " '01111001 01110011',\n",
       " '011299',\n",
       " '011299 public',\n",
       " '0115',\n",
       " '0115 wheres',\n",
       " '011607',\n",
       " '011607 think',\n",
       " '0117',\n",
       " '0117 4452271145527',\n",
       " '011720',\n",
       " '011720 pm',\n",
       " '011785',\n",
       " '011785 45227',\n",
       " '012',\n",
       " '012 born',\n",
       " '012 hair',\n",
       " '012 matter',\n",
       " '012 wear',\n",
       " '0120',\n",
       " '0120 months',\n",
       " '012001',\n",
       " '012001 bush',\n",
       " '01222004',\n",
       " '01222004 janklow',\n",
       " '0122928688',\n",
       " '0122928688 home',\n",
       " '012413',\n",
       " '012413 google',\n",
       " '0124142690',\n",
       " '0124142690 copyright',\n",
       " '0127',\n",
       " '0127 mm',\n",
       " '013',\n",
       " '013 born',\n",
       " '013 braces',\n",
       " '013 reside',\n",
       " '0130',\n",
       " '0130 stayed',\n",
       " '0134',\n",
       " '0134 jumper991',\n",
       " '0137',\n",
       " '0137 yesterday',\n",
       " '013micron',\n",
       " '013micron process',\n",
       " '014',\n",
       " '014 age',\n",
       " '014 current',\n",
       " '014 hair',\n",
       " '014 middle',\n",
       " '0140296409',\n",
       " '0140296409 disgrace',\n",
       " '0140hrs',\n",
       " '0140hrs im',\n",
       " '0142001430',\n",
       " '0142001430 itinerant',\n",
       " '0142001740',\n",
       " '0142001740 secret',\n",
       " '0142001805',\n",
       " '0142001805 imagine',\n",
       " '0142001821',\n",
       " '0142001821 vibrant',\n",
       " '0142002887',\n",
       " '0142002887 family',\n",
       " '014200331x',\n",
       " '014200331x nineyearold',\n",
       " '0142004235',\n",
       " '0142004235 classic',\n",
       " '0145',\n",
       " '015',\n",
       " '015 born',\n",
       " '015 percent',\n",
       " '015 screen',\n",
       " '015 zodiac',\n",
       " '0152',\n",
       " '0155',\n",
       " '0155 way',\n",
       " '0156027321',\n",
       " '0156027321 16yearold',\n",
       " '0156028778',\n",
       " '0156028778 scope',\n",
       " '015605',\n",
       " '015605 0700',\n",
       " '015lb',\n",
       " '015lb theyll',\n",
       " '016',\n",
       " '016 current',\n",
       " '016 email',\n",
       " '016 many',\n",
       " '016 mini',\n",
       " '016x',\n",
       " '016x sprintfformat',\n",
       " '017',\n",
       " '017 language',\n",
       " '017 laungage',\n",
       " '017 screen',\n",
       " '017 zodiac',\n",
       " '0171',\n",
       " '0171 nbspnbspnbspnbspnbspnbspnbspnbsp',\n",
       " '01752',\n",
       " '01752 224840',\n",
       " '0178595',\n",
       " '0178595 cw111',\n",
       " '018',\n",
       " '018 bad',\n",
       " '018 blog',\n",
       " '018 blurty',\n",
       " '018 many',\n",
       " '019',\n",
       " '019 diary',\n",
       " '019 nationality',\n",
       " '019 piercing',\n",
       " '0191',\n",
       " '0191 232',\n",
       " '0194447132',\n",
       " '0194447132 birthday',\n",
       " '01db',\n",
       " '01db spent',\n",
       " '01l',\n",
       " '01l clarrisa',\n",
       " '01l fall',\n",
       " '01l geometry',\n",
       " '01l god',\n",
       " '01l pictures',\n",
       " '01l red',\n",
       " '01l shut',\n",
       " '01l water',\n",
       " '01lifestyles',\n",
       " '01lifestyles rich',\n",
       " '01make',\n",
       " '01make sure',\n",
       " '01nbsp',\n",
       " '01nbsp check',\n",
       " '01nbsp new',\n",
       " '01nbspnbsp',\n",
       " '01nbspnbsp new',\n",
       " '01nov',\n",
       " '01nov 30',\n",
       " '01something',\n",
       " '01something ian',\n",
       " '02',\n",
       " '02 2000',\n",
       " '02 2002',\n",
       " '02 2004',\n",
       " '02 2k3',\n",
       " '02 40',\n",
       " '02 algorithm',\n",
       " '02 also',\n",
       " '02 another',\n",
       " '02 argue',\n",
       " '02 bad',\n",
       " '02 barry',\n",
       " '02 basically',\n",
       " '02 best',\n",
       " '02 blast',\n",
       " '02 book',\n",
       " '02 bought',\n",
       " '02 brusstar',\n",
       " '02 caused',\n",
       " '02 certain',\n",
       " '02 color',\n",
       " '02 colorchanging',\n",
       " '02 consideration',\n",
       " '02 continued',\n",
       " '02 cool',\n",
       " '02 daniel',\n",
       " '02 dave',\n",
       " '02 deadbeat',\n",
       " '02 diablo',\n",
       " '02 didnt',\n",
       " '02 dismantle',\n",
       " '02 dogs',\n",
       " '02 dont',\n",
       " '02 drop',\n",
       " '02 early',\n",
       " '02 empathetic',\n",
       " '02 encore',\n",
       " '02 erotica_writings',\n",
       " '02 falling',\n",
       " '02 fear',\n",
       " '02 first',\n",
       " '02 forgetting',\n",
       " '02 girls',\n",
       " '02 good',\n",
       " '02 governor',\n",
       " '02 hello',\n",
       " '02 holding',\n",
       " '02 holiday',\n",
       " '02 hr',\n",
       " '02 im',\n",
       " '02 increase',\n",
       " '02 intended',\n",
       " '02 interesting',\n",
       " '02 johnny',\n",
       " '02 jump',\n",
       " '02 june',\n",
       " '02 last',\n",
       " '02 letters',\n",
       " '02 like',\n",
       " '02 listening',\n",
       " '02 lotion',\n",
       " '02 martial',\n",
       " '02 matt',\n",
       " '02 meaning',\n",
       " '02 middle',\n",
       " '02 miles',\n",
       " '02 miramax',\n",
       " '02 mom',\n",
       " '02 morning',\n",
       " '02 movies',\n",
       " '02 name',\n",
       " '02 new',\n",
       " '02 night',\n",
       " '02 nissan',\n",
       " '02 one',\n",
       " '02 paul',\n",
       " '02 people',\n",
       " '02 percent',\n",
       " '02 pet',\n",
       " '02 pizza',\n",
       " '02 play',\n",
       " '02 playing',\n",
       " '02 polls',\n",
       " '02 queer',\n",
       " '02 reggaecolored',\n",
       " '02 ride',\n",
       " '02 room',\n",
       " '02 sandy',\n",
       " '02 say910',\n",
       " '02 season',\n",
       " '02 seconds',\n",
       " '02 shorts',\n",
       " '02 sometimes',\n",
       " '02 spent',\n",
       " '02 survivor',\n",
       " '02 thomas',\n",
       " '02 throws',\n",
       " '02 time',\n",
       " '02 took',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64f5ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3019c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f36b0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indUnk                     35141\n",
       "Student                    21548\n",
       "Technology                  5981\n",
       "Arts                        4534\n",
       "Education                   4066\n",
       "Communications-Media        2809\n",
       "Internet                    2254\n",
       "Non-Profit                  2072\n",
       "Engineering                 1658\n",
       "Law                         1217\n",
       "Publishing                  1128\n",
       "Science                     1006\n",
       "Government                   997\n",
       "Consulting                   836\n",
       "Religion                     722\n",
       "Marketing                    691\n",
       "Advertising                  681\n",
       "Fashion                      666\n",
       "BusinessServices             589\n",
       "Banking                      553\n",
       "Chemicals                    553\n",
       "Accounting                   541\n",
       "Telecommunications           534\n",
       "Military                     468\n",
       "Sports-Recreation            452\n",
       "Museums-Libraries            449\n",
       "HumanResources               421\n",
       "RealEstate                   400\n",
       "Manufacturing                360\n",
       "Transportation               332\n",
       "Biotech                      293\n",
       "Tourism                      286\n",
       "LawEnforcement-Security      276\n",
       "Architecture                 214\n",
       "Agriculture                  178\n",
       "InvestmentBanking            167\n",
       "Automotive                   142\n",
       "Construction                 136\n",
       "Environment                   85\n",
       "Maritime                      36\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102eca9",
   "metadata": {},
   "source": [
    "Transform Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbc129a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd234eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34,  4, 14, ..., 39, 21, 34])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91664f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39,  3, 34, ..., 34, 39, 39])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab790b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373712               Student\n",
       "342773                  Arts\n",
       "257659           Engineering\n",
       "537473              Internet\n",
       "442852    Telecommunications\n",
       "                 ...        \n",
       "51834                 indUnk\n",
       "300534            Publishing\n",
       "458420                indUnk\n",
       "653566                   Law\n",
       "286758               Student\n",
       "Name: topic, Length: 76377, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e4ec4",
   "metadata": {},
   "source": [
    "Build a base model for Supervised Learning - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "049138a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', min_samples_leaf=3, random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel1 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', min_samples_leaf = 3, random_state = 42)\n",
    "\n",
    "rfmodel1.fit(X_train_cv, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afd59e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfmodel1.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe75945",
   "metadata": {},
   "source": [
    "Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2293a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37083006022518983\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test_enc, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c70f5761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        97\n",
      "           1       0.00      0.00      0.00       109\n",
      "           2       0.00      0.00      0.00        30\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.00      0.00      0.00       923\n",
      "           5       0.00      0.00      0.00        16\n",
      "           6       0.00      0.00      0.00       106\n",
      "           7       0.00      0.00      0.00        58\n",
      "           8       0.00      0.00      0.00       104\n",
      "           9       0.00      0.00      0.00       113\n",
      "          10       0.00      0.00      0.00       601\n",
      "          11       0.00      0.00      0.00        28\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.00      0.00      0.00       853\n",
      "          14       1.00      0.00      0.01       341\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       1.00      0.01      0.02       107\n",
      "          17       0.00      0.00      0.00       211\n",
      "          18       0.00      0.00      0.00        69\n",
      "          19       1.00      0.00      0.01       434\n",
      "          20       0.00      0.00      0.00        32\n",
      "          21       0.00      0.00      0.00       231\n",
      "          22       0.00      0.00      0.00        63\n",
      "          23       0.00      0.00      0.00        81\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00       142\n",
      "          26       0.00      0.00      0.00        85\n",
      "          27       0.00      0.00      0.00        84\n",
      "          28       0.00      0.00      0.00       384\n",
      "          29       0.00      0.00      0.00       198\n",
      "          30       0.00      0.00      0.00        89\n",
      "          31       0.00      0.00      0.00       148\n",
      "          32       0.00      0.00      0.00       200\n",
      "          33       0.00      0.00      0.00        76\n",
      "          34       0.58      0.00      0.00      4377\n",
      "          35       0.00      0.00      0.00      1178\n",
      "          36       0.00      0.00      0.00       123\n",
      "          37       0.00      0.00      0.00        62\n",
      "          38       1.00      0.16      0.28        56\n",
      "          39       0.37      1.00      0.54      7066\n",
      "\n",
      "    accuracy                           0.37     19095\n",
      "   macro avg       0.12      0.03      0.02     19095\n",
      "weighted avg       0.32      0.37      0.20     19095\n",
      "\n",
      "[[   0    0    0 ...    0    0   97]\n",
      " [   0    0    0 ...    0    0  109]\n",
      " [   0    0    0 ...    0    0   30]\n",
      " ...\n",
      " [   0    0    0 ...    0    0   62]\n",
      " [   0    0    0 ...    0    9   47]\n",
      " [   0    0    0 ...    0    0 7061]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_enc, y_pred))\n",
    "cm = confusion_matrix(y_test_enc, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2220f85",
   "metadata": {},
   "source": [
    "TF - IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c1ebd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(max_features = 200)\n",
    "\n",
    "tf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa76847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'tomorrow': 172, 'need': 113, 'also': 2, 'everything': 40, 'must': 110, 'im': 76, 'going': 56, 'wont': 190, 'let': 86, 'post': 130, 'first': 44, 'part': 124, 'blog': 17, 'dont': 32, 'read': 135, 'want': 181, 'today': 170, 'times': 169, 'much': 108, 'hate': 66, 'job': 78, 'school': 144, 'thinking': 164, 'guess': 61, 'maybe': 102, 'getting': 51, 'people': 125, 'things': 162, 'believe': 12, 'another': 4, 'life': 87, 'morning': 106, 'day': 27, 'home': 70, 'found': 45, 'one': 123, 'work': 191, 'came': 20, 'hope': 71, 'doesnt': 30, 'come': 23, 'make': 97, 'anyone': 5, 'feel': 41, 'bad': 11, 'actually': 0, 'friends': 47, 'find': 43, 'someone': 148, 'still': 152, 'ill': 75, 'went': 187, 'good': 58, 'made': 96, 'friend': 46, 'house': 73, 'nice': 117, 'see': 145, 'old': 122, 'well': 186, 'back': 10, 'game': 49, 'night': 118, 'last': 82, 'week': 185, 'got': 59, 'looking': 93, 'sure': 154, 'like': 88, 'real': 136, 'big': 15, 'next': 116, 'year': 195, 'play': 128, 'ive': 77, 'urllink': 177, 'may': 101, 'new': 115, 'finally': 42, 'around': 8, 'two': 176, 'point': 129, 'enough': 35, 'happy': 64, 'think': 163, 'really': 137, 'would': 193, 'take': 155, 'call': 18, 'many': 100, 'said': 141, 'kind': 80, 'yes': 197, 'thats': 159, 'time': 168, 'though': 165, 'give': 53, 'end': 34, 'right': 139, 'thing': 161, 'always': 3, 'try': 174, 'best': 13, 'help': 68, 'least': 84, 'little': 89, 'know': 81, 'get': 50, 'wanted': 182, 'say': 143, 'something': 149, 'else': 33, 'could': 25, 'put': 133, 'music': 109, 'whole': 188, 'away': 9, 'way': 184, 'used': 180, 'oh': 120, 'use': 179, 'even': 36, 'every': 38, 'mean': 103, 'look': 92, 'go': 54, 'tell': 158, 'fun': 48, 'cool': 24, 'head': 67, 'talking': 157, 'quite': 134, 'might': 104, 'start': 150, 'anything': 6, 'three': 167, 'hours': 72, 'ok': 121, 'left': 85, 'bit': 16, 'trying': 175, 'ever': 37, 'wasnt': 183, 'hes': 69, 'man': 99, 'never': 114, 'already': 1, 'gonna': 57, 'god': 55, 'yet': 198, 'probably': 132, 'told': 171, 'great': 60, 'live': 90, 'yeah': 194, 'mind': 105, 'show': 146, 'since': 147, 'took': 173, 'love': 95, 'place': 127, 'guys': 63, 'anyway': 7, 'better': 14, 'keep': 79, 'remember': 138, 'youre': 199, 'world': 192, 'pretty': 131, 'stuff': 153, 'cant': 21, 'nothing': 119, 'us': 178, 'person': 126, 'hard': 65, 'girl': 52, 'done': 31, 'lot': 94, 'long': 91, 'didnt': 29, 'thought': 166, 'room': 140, 'saw': 142, 'movie': 107, 'called': 19, 'without': 189, 'years': 196, 'started': 151, 'later': 83, 'talk': 156, 'days': 28, 'makes': 98, 'course': 26, 'guy': 62, 'id': 74, 'theres': 160, 'everyone': 39, 'name': 111, 'car': 22, 'nbsp': 112}\n"
     ]
    }
   ],
   "source": [
    "print('\\nWord indexes:')\n",
    "print(tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "889566d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tf.transform(X_train)\n",
    "X_test_tf = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fcaf688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', min_samples_leaf=3, random_state=42)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel2 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', min_samples_leaf = 3, random_state = 42)\n",
    "\n",
    "rfmodel2.fit(X_train_tf, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e11c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tf = rfmodel2.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dbce1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3739198743126473\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test_enc, y_pred_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7614a79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        97\n",
      "           1       0.00      0.00      0.00       109\n",
      "           2       0.00      0.00      0.00        30\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.29      0.01      0.01       923\n",
      "           5       0.00      0.00      0.00        16\n",
      "           6       0.00      0.00      0.00       106\n",
      "           7       0.00      0.00      0.00        58\n",
      "           8       0.00      0.00      0.00       104\n",
      "           9       0.00      0.00      0.00       113\n",
      "          10       0.00      0.00      0.00       601\n",
      "          11       0.00      0.00      0.00        28\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.25      0.00      0.00       853\n",
      "          14       0.00      0.00      0.00       341\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       0.00      0.00      0.00       107\n",
      "          17       0.00      0.00      0.00       211\n",
      "          18       0.00      0.00      0.00        69\n",
      "          19       0.50      0.00      0.01       434\n",
      "          20       0.00      0.00      0.00        32\n",
      "          21       0.00      0.00      0.00       231\n",
      "          22       0.00      0.00      0.00        63\n",
      "          23       0.00      0.00      0.00        81\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00       142\n",
      "          26       0.00      0.00      0.00        85\n",
      "          27       0.00      0.00      0.00        84\n",
      "          28       0.00      0.00      0.00       384\n",
      "          29       0.00      0.00      0.00       198\n",
      "          30       0.00      0.00      0.00        89\n",
      "          31       0.00      0.00      0.00       148\n",
      "          32       0.00      0.00      0.00       200\n",
      "          33       0.00      0.00      0.00        76\n",
      "          34       0.40      0.12      0.19      4377\n",
      "          35       0.28      0.00      0.01      1178\n",
      "          36       0.00      0.00      0.00       123\n",
      "          37       0.00      0.00      0.00        62\n",
      "          38       1.00      0.07      0.13        56\n",
      "          39       0.37      0.93      0.53      7066\n",
      "\n",
      "    accuracy                           0.37     19095\n",
      "   macro avg       0.08      0.03      0.02     19095\n",
      "weighted avg       0.29      0.37      0.24     19095\n",
      "\n",
      "[[   0    0    0 ...    0    0   93]\n",
      " [   0    0    0 ...    0    0  105]\n",
      " [   0    0    0 ...    0    0   29]\n",
      " ...\n",
      " [   0    0    0 ...    0    0   56]\n",
      " [   0    0    0 ...    0    4   50]\n",
      " [   0    0    0 ...    0    0 6581]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_enc, y_pred_tf))\n",
    "cm = confusion_matrix(y_test_enc, y_pred_tf)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d05790a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C=1.0, penalty='l1', dual=False, loss='squared_hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03d101c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_cv = svc.fit(X_train_cv, y_train_enc)\n",
    "y_pred_svccv = svc_cv.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f2614aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3473684210526316\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test_enc, y_pred_svccv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d612cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.14      0.17        97\n",
      "           1       0.10      0.08      0.09       109\n",
      "           2       0.11      0.07      0.08        30\n",
      "           3       0.24      0.09      0.13        43\n",
      "           4       0.19      0.14      0.16       923\n",
      "           5       0.00      0.00      0.00        16\n",
      "           6       0.23      0.13      0.17       106\n",
      "           7       0.10      0.03      0.05        58\n",
      "           8       0.19      0.14      0.16       104\n",
      "           9       0.04      0.02      0.02       113\n",
      "          10       0.14      0.09      0.11       601\n",
      "          11       0.15      0.07      0.10        28\n",
      "          12       0.15      0.09      0.12       158\n",
      "          13       0.21      0.15      0.17       853\n",
      "          14       0.19      0.11      0.14       341\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       0.34      0.25      0.29       107\n",
      "          17       0.16      0.09      0.11       211\n",
      "          18       0.07      0.04      0.06        69\n",
      "          19       0.13      0.08      0.10       434\n",
      "          20       0.06      0.03      0.04        32\n",
      "          21       0.14      0.10      0.12       231\n",
      "          22       0.36      0.13      0.19        63\n",
      "          23       0.26      0.09      0.13        81\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.15      0.09      0.11       142\n",
      "          26       0.14      0.08      0.10        85\n",
      "          27       0.18      0.13      0.15        84\n",
      "          28       0.13      0.08      0.10       384\n",
      "          29       0.18      0.14      0.16       198\n",
      "          30       0.06      0.03      0.04        89\n",
      "          31       0.18      0.09      0.12       148\n",
      "          32       0.13      0.07      0.10       200\n",
      "          33       0.15      0.08      0.10        76\n",
      "          34       0.38      0.40      0.39      4377\n",
      "          35       0.22      0.18      0.20      1178\n",
      "          36       0.17      0.09      0.12       123\n",
      "          37       0.23      0.13      0.16        62\n",
      "          38       0.34      0.21      0.26        56\n",
      "          39       0.42      0.56      0.48      7066\n",
      "\n",
      "    accuracy                           0.35     19095\n",
      "   macro avg       0.17      0.11      0.13     19095\n",
      "weighted avg       0.31      0.35      0.32     19095\n",
      "\n",
      "[[  14    1    0 ...    0    0   38]\n",
      " [   0    9    0 ...    0    1   48]\n",
      " [   1    0    2 ...    0    0   12]\n",
      " ...\n",
      " [   0    0    0 ...    8    0   25]\n",
      " [   0    0    0 ...    0   12   17]\n",
      " [  26   36    6 ...   13    6 3953]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_enc, y_pred_svccv))\n",
    "cm = confusion_matrix(y_test_enc, y_pred_svccv)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93b03682",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tf = svc.fit(X_train_tf, y_train_enc)\n",
    "y_pred_svctf = svc_tf.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d551f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3799423932966745\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test_enc, y_pred_svctf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f7b252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        97\n",
      "           1       0.00      0.00      0.00       109\n",
      "           2       0.00      0.00      0.00        30\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.00      0.00      0.00       923\n",
      "           5       0.00      0.00      0.00        16\n",
      "           6       0.00      0.00      0.00       106\n",
      "           7       0.00      0.00      0.00        58\n",
      "           8       0.00      0.00      0.00       104\n",
      "           9       0.00      0.00      0.00       113\n",
      "          10       0.00      0.00      0.00       601\n",
      "          11       0.00      0.00      0.00        28\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.00      0.00      0.00       853\n",
      "          14       0.00      0.00      0.00       341\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       0.00      0.00      0.00       107\n",
      "          17       0.00      0.00      0.00       211\n",
      "          18       0.00      0.00      0.00        69\n",
      "          19       0.00      0.00      0.00       434\n",
      "          20       0.00      0.00      0.00        32\n",
      "          21       0.00      0.00      0.00       231\n",
      "          22       0.00      0.00      0.00        63\n",
      "          23       0.00      0.00      0.00        81\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00       142\n",
      "          26       0.00      0.00      0.00        85\n",
      "          27       0.00      0.00      0.00        84\n",
      "          28       0.00      0.00      0.00       384\n",
      "          29       0.00      0.00      0.00       198\n",
      "          30       0.00      0.00      0.00        89\n",
      "          31       0.00      0.00      0.00       148\n",
      "          32       0.00      0.00      0.00       200\n",
      "          33       0.00      0.00      0.00        76\n",
      "          34       0.41      0.21      0.28      4377\n",
      "          35       0.00      0.00      0.00      1178\n",
      "          36       0.00      0.00      0.00       123\n",
      "          37       0.00      0.00      0.00        62\n",
      "          38       0.00      0.00      0.00        56\n",
      "          39       0.38      0.90      0.53      7066\n",
      "\n",
      "    accuracy                           0.38     19095\n",
      "   macro avg       0.02      0.03      0.02     19095\n",
      "weighted avg       0.23      0.38      0.26     19095\n",
      "\n",
      "[[   0    0    0 ...    0    0   90]\n",
      " [   0    0    0 ...    0    0   98]\n",
      " [   0    0    0 ...    0    0   28]\n",
      " ...\n",
      " [   0    0    0 ...    0    0   49]\n",
      " [   0    0    0 ...    0    0   52]\n",
      " [   0    0    0 ...    0    0 6338]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_enc, y_pred_svctf))\n",
    "cm = confusion_matrix(y_test_enc, y_pred_svctf)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd12a57",
   "metadata": {},
   "source": [
    "Performance Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844fd66",
   "metadata": {},
   "source": [
    "For the Random Forest model, both Count and TF-IDF vectorizers gave similar accuracy results of 37%, the weighted precision and recall were marginally better for the Count vectorizer. For SVM, Count vectorizer has given a lower accuracy of 34% compared to an accuracy of 38% for the TF-IDF vectorizer. However, precision of the count vectorizer model (0.31) is better than that of the TF-IDF model (0.23) while the recall is lower. Overall, both count vectorizer and TF-IDF performed more or less similar.\n",
    "\n",
    "SVM model on TF-IDF vectors outperformed with 38% accuracy and good weighted average precision and recall. SVM performed better than RandomForest because I have kept the hyperparameter min_samples_leaf = 3 for RF model which ensures the model runs faster but is not the most accurate. SVM, on the other hand had an 'L1' regularization with C = 1, so moderate regularization.\n",
    "\n",
    "SVM with 'L1' regularization and C = 1 improved the performance. The model ran relatively fast and also gave good accuracy. Larger C would have led to overfitting while lower C might have led to underfitting.\n",
    "\n",
    "We have an imbalanced dataset but we want to assign greater contribution to classes with more examples in the dataset, so the weighted average precision and recall values are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784f6e4",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78346cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0fea8c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'Intro', 'patterns': ['hi', 'how are you', 'is anyone there', 'hello', 'whats up', 'hey', 'yo', 'listen', 'please help me', 'i am learner from', 'i belong to', 'aiml batch', 'aifl batch', 'i am from', 'my pm is', 'blended', 'online', 'i am from', 'please guide me', 'hey ya', 'talking to you for first time'], 'responses': ['Hello! how can i help you ?', 'Hello! How may I assist you ?'], 'context_set': ''}, {'tag': 'Exit', 'patterns': ['thank you', 'thanks', 'cya', 'see you', 'later', 'see you later', 'goodbye', 'i am leaving', 'have a Good day', 'you helped me', 'thanks a lot', 'thanks a ton', 'you are the best', 'great help', 'too good', 'you are a good learning buddy'], 'responses': ['I hope I was able to assist you, Good Bye', 'Good Bye, I hope your query is resolved'], 'context_set': ''}, {'tag': 'Olympus', 'patterns': ['olympus', 'explain me how olympus works', 'I am not able to understand olympus', 'olympus window not working', 'no access to olympus', 'unable to see link in olympus', 'no link visible on olympus', 'whom to contact for olympus', 'lot of problem with olympus', 'olympus is not a good tool', 'lot of problems with olympus', 'how to use olympus', 'teach me olympus'], 'responses': ['Link: Olympus wiki', 'Connect with your Program Manager to access Olympus the learning portal'], 'context_set': ''}, {'tag': 'SL', 'patterns': ['i am not able to understand svm', 'explain me how machine learning works', 'i am not able to understand naive bayes', 'i am not able to understand logistic regression', 'i am not able to understand ensemble techb=niques', 'i am not able to understand knn', 'i am not able to understand knn imputer', 'i am not able to understand cross validation', 'i am not able to understand boosting', 'i am not able to understand random forest', 'i am not able to understand ada boosting', 'i am not able to understand gradient boosting', 'machine learning', 'ML', 'SL', 'supervised learning', 'knn', 'logistic regression', 'regression', 'classification', 'naive bayes', 'nb', 'ensemble techniques', 'bagging', 'boosting', 'ada boosting', 'ada', 'gradient boosting', 'hyper parameters'], 'responses': ['Link: Machine Learning wiki ', 'Go to the Machine Learning module in Olympus portal'], 'context_set': ''}, {'tag': 'NN', 'patterns': ['what is deep learning', 'unable to understand deep learning', 'explain me how deep learning works', 'i am not able to understand deep learning', 'not able to understand neural nets', 'very diffult to understand neural nets', 'unable to understand neural nets', 'ann', 'artificial intelligence', 'artificial neural networks', 'weights', 'activation function', 'hidden layers', 'softmax', 'sigmoid', 'relu', 'otimizer', 'forward propagation', 'backward propagation', 'epochs', 'epoch', 'what is an epoch', 'adam', 'sgd'], 'responses': ['Link: Neural Nets wiki', 'Go to the Neural Network Section in Olympus portal'], 'context_set': ''}, {'tag': 'Bot', 'patterns': ['what is your name', 'who are you', 'name please', 'when are your hours of opertions', 'what are your working hours', 'hours of operation', 'working hours', 'hours'], 'responses': ['I am your virtual learning assistant'], 'context_set': ''}, {'tag': 'Profane', 'patterns': ['what the hell', 'bloody stupid bot', 'do you think you are very smart', 'screw you', 'i hate you', 'you are stupid', 'jerk', 'you are a joke', 'useless piece of shit'], 'responses': ['Please use respectful words'], 'context_set': ''}, {'tag': 'Ticket', 'patterns': ['my problem is not solved', 'you did not help me', 'not a good solution', 'bad solution', 'not good solution', 'no help', 'wasted my time', 'useless bot', 'create a ticket'], 'responses': ['Transferring the request to your PM'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "with open('GLBot.json') as file:\n",
    "    corpus = json.load(file)\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57b64a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3bd44519",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [] \n",
    "L = [] \n",
    "doc_x = [] \n",
    "doc_y = [] \n",
    "\n",
    "for intent in corpus['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w_temp = nltk.word_tokenize(pattern)\n",
    "        W.extend(w_temp)\n",
    "        doc_x.append(w_temp)\n",
    "        doc_y.append(intent['tag'])\n",
    "        \n",
    "    if intent['tag'] not in L:\n",
    "        L.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb5e11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    "W = [stemmer.stem(w.lower()) for w in W if w != '?'] #stemming or learning the root word\n",
    "W = sorted(list(set(W))) #sorted words\n",
    "L = sorted(L) #sorted list of tags or labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a26cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = []\n",
    "Target = []\n",
    "out_empty = [0 for _ in range(len(L))]\n",
    "\n",
    "for x, doc in enumerate(doc_x):\n",
    "    bag=[]\n",
    "    \n",
    "    w_temp = [stemmer.stem(w.lower()) for w in doc]\n",
    "    \n",
    "    for w in W:\n",
    "        if w in w_temp:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "            \n",
    "    output_row = out_empty[:]\n",
    "    output_row[L.index(doc_y[x])] = 1\n",
    "    \n",
    "    Train.append(bag)\n",
    "    Target.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7e2f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 128)               19840     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,440\n",
      "Trainable params: 30,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 8ms/step - loss: 2.0305 - accuracy: 0.1628\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.7051 - accuracy: 0.4806\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.3403 - accuracy: 0.6667\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8628 - accuracy: 0.7752\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5198 - accuracy: 0.8605\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.9147\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1035 - accuracy: 0.9845\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.6521e-04 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.9841e-04 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.5456e-04 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.8436e-04 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2349e-04 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.9721e-04 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7480e-04 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.6177e-04 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4880e-04 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4018e-04 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3067e-04 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.2260e-04 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1099e-04 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1058e-04 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0794e-04 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.0561e-04 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0180e-04 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.8086e-05 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9.3513e-05 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.9457e-05 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.5800e-05 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.2254e-05 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7.8943e-05 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7.5770e-05 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.2820e-05 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 7.0372e-05 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6.8104e-05 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6.5582e-05 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.3417e-05 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.1256e-05 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5.9416e-05 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 5.7804e-05 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.6048e-05 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 5.4570e-05 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 5.2678e-05 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5.1163e-05 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.9714e-05 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.8659e-05 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.7295e-05 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.6206e-05 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.5199e-05 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.4248e-05 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.3306e-05 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.2264e-05 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.1399e-05 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.0589e-05 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.9683e-05 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.8852e-05 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.8046e-05 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.7214e-05 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.6561e-05 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.5952e-05 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.5217e-05 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.4696e-05 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.4235e-05 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.3693e-05 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.3134e-05 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.2530e-05 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.1936e-05 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.1363e-05 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.0857e-05 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.0310e-05 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.9697e-05 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.9218e-05 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.8671e-05 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.8149e-05 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7705e-05 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7281e-05 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6802e-05 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6387e-05 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.5914e-05 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.5561e-05 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.5135e-05 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4713e-05 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.4233e-05 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.3911e-05 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3446e-05 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.3059e-05 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2645e-05 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.2260e-05 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.1910e-05 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.1669e-05 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.1317e-05 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.1034e-05 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.0755e-05 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.0437e-05 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.0137e-05 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9863e-05 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.9580e-05 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.9273e-05 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8992e-05 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.8724e-05 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.8412e-05 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.8172e-05 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7934e-05 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7692e-05 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7469e-05 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7261e-05 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.7052e-05 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.6820e-05 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.6603e-05 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.6372e-05 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.6166e-05 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5931e-05 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5726e-05 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.5519e-05 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.5326e-05 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.5131e-05 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.4971e-05 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4814e-05 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4618e-05 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4458e-05 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4293e-05 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4141e-05 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3982e-05 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.3825e-05 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3694e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.3537e-05 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3387e-05 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3245e-05 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3108e-05 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.2943e-05 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2813e-05 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2649e-05 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.2506e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2351e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.2198e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.2041e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.1877e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.1750e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.1597e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1487e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.1348e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1208e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.1080e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.0974e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.0865e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0739e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0624e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0518e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0399e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0306e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 954us/step - loss: 1.0205e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0111e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0018e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.9153e-06 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 9.8229e-06 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.7249e-06 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.6353e-06 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.5373e-06 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9.4255e-06 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.3119e-06 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.2315e-06 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9.1298e-06 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.0550e-06 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.9681e-06 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 8.8840e-06 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8.8046e-06 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.7297e-06 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.6438e-06 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.5745e-06 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8.4701e-06 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.3749e-06 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.2963e-06 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.2169e-06 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8.0635e-06 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.8879e-06 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.7724e-06 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.6643e-06 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.5775e-06 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.4980e-06 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.4213e-06 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.3548e-06 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7.2697e-06 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.1875e-06 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7.1071e-06 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7.0443e-06 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6.9870e-06 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.9204e-06 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.8678e-06 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.8077e-06 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6.7532e-06 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.6959e-06 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.6395e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6.5813e-06 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.5203e-06 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6.4612e-06 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6.4094e-06 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 6.3642e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23405d809d0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers\n",
    "\n",
    "nnmodel = models.Sequential()\n",
    "nnmodel.add(layers.Dense(128, input_dim = len(Train[0]), activation = 'relu'))\n",
    "nnmodel.add(layers.Dense(64, activation = 'relu'))\n",
    "nnmodel.add(layers.Dense(32, activation = 'relu'))\n",
    "nnmodel.add(layers.Dense(len(Target[0]), activation = 'softmax'))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "nnmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "print(nnmodel.summary())\n",
    "\n",
    "# Training the model\n",
    "nnmodel.fit(Train, Target, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "679ee351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_lemmatize(text): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = tokenize_lemmatize(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94bd5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_label(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = nnmodel.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6fa8c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT : Chat with the bot[Type 'quit' to stop] !\n",
      "\n",
      "BOT : If answer is not  right[Type '*'] !\n",
      "\n",
      "\n",
      "You: hello\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\n",
      "BOT :  Hello! How may I assist you ?\n",
      "\n",
      "\n",
      "You: who are you\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\n",
      "BOT :  I am your virtual learning assistant\n",
      "\n",
      "\n",
      "You: access to olympus\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "\n",
      "BOT :  Connect with your Program Manager to access Olympus the learning portal\n",
      "\n",
      "\n",
      "You: not able to understand ada boosting\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "\n",
      "BOT :  Link: Machine Learning wiki \n",
      "\n",
      "\n",
      "You: what is neural network\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "\n",
      "BOT :  Link: Neural Nets wiki\n",
      "\n",
      "\n",
      "You: *\n",
      "\n",
      "BOT:Please rephrase your question and try again\n",
      "\n",
      "\n",
      "You: stupid\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "\n",
      "BOT :  Please use respectful words\n",
      "\n",
      "\n",
      "You: you did not help me\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "\n",
      "BOT :  Transferring the request to your PM\n",
      "\n",
      "\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "# Running the chatbot\n",
    "print(\"BOT : Chat with the bot[Type 'quit' to stop] !\")\n",
    "print(\"\\nBOT : If answer is not  right[Type '*'] !\")\n",
    "while True:\n",
    "\n",
    "    inp = input(\"\\n\\nYou: \")\n",
    "\n",
    "    if inp.lower() == \"*\":\n",
    "        print(\"\\nBOT:Please rephrase your question and try again\")\n",
    "\n",
    "    if inp.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if inp.lower() != \"*\":\n",
    "        intents = pred_label(inp, W, L)\n",
    "        tag = intents[0]\n",
    "        list_of_intents = corpus[\"intents\"]\n",
    "        for i in list_of_intents: \n",
    "            if i[\"tag\"] == tag:\n",
    "                result = random.choice(i[\"responses\"])\n",
    "                break\n",
    "        print(\"\\nBOT : \", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
